<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" />

<style>
body {
    margin: 10px;
    padding: 10px;
/*     border: 1px solid #cccccc; */
    max-width: 1200px; /* Optional: Set a max-width for the content */
    margin-left: auto;
    margin-right: auto;
    background-color: #f9f9f9; /* Optional: Change the background color */
}
</style>
<title>Da Chang</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Da Chang</h1>
</div>
<table class="imgtable"><tr><td>
<img src="img/me.jpg" alt="alt text" width="180px" height="240px" />&nbsp;</td>
<td align="left"><p>Da Chang(昌达)<br />
Ph.D student,<br />
Pengcheng Laboratory,Shenzhen,China <br />
Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China</p>
<p>
<a href="changda24@mails.ucas.ac.cn">[Email]</a> 
<a href="https://scholar.google.com.hk/citations?user=glTBszIAAAAJ&hl=zh-CN">[Google Scholar]</a> 
<a href="https://www.linkedin.com/in/da-chang-4a13262b8/">[Linkedin]</a> 
<a href="https://github.com/VceChang">[Github]</a> 

</p>
</td></tr></table>
<h2>About me</h2>
<p style="line-height: 1.8;">
I graduated from the Department of Intelligent Science and Technology, <a href="https://soa.csu.edu.cn/">School of Automation</a> ,<a href="https://www.csu.edu.cn/"> Central South University</a>.<br/>
Currently, I am a jointly educated PhD candidate in a collaborative program between the Shenzhen Institute of Advanced Technology,Chinese Academy of Sciences (<a href="https://www.siat.ac.cn/">SIAT</a>) and Pengcheng Laboratory(<a href="https://www.pcl.ac.cn/">PCL</a>).<br/> 
	
My research interests focus on deep learning optimization and generalization and the application of deep models to various domains. <br/>
I am very interested in the theory and application of deep learning. I would like to communicate with you about training techniques, application scenarios and optimization theories of deep learning.
 
</p>

<h2>News</h2>

<ul><li><p>[09/2024] - I will start my first year of doctoral study in Chinese Academy of Sciences</p></li></ul>
<ul><li><p>[06/2024] - I graduated from the School of Automation of Central South University.</p></li></ul>
<ul><li><p>[06/2024] - I won the outstanding undergraduate thesis of Central South University.</p></li></ul>

<h2>Research that I lead </h2>
<!-- <img src="img/me.jpg" alt="alt text" width="180px" height="240px" />&nbsp;</td>
<p><a href="https://arxiv.org/abs/2404.12734">DLoRA-TrOCR: Mixed Text Mode Optical Character Recognition Based On Transformer</a> <br />
<b>Da Chang</b>, Yu Li <br />
</p> -->

<table style="width: 100%; border-collapse: collapse;border: none;">
    <tr>
        <td style="width: 35%;text-align: center; vertical-align: middle; padding-right: 20px;border: none;">
            <img src="img/DLoRA.png" alt="DLoRA" style="max-width: 80%; height: auto;">
        </td>
         
        <td style="width: 65%; vertical-align: top;text-align: left;border: none;">
           <h3 style="line-height: 1.5;">DLoRA-TrOCR: Mixed Text Mode Optical Character Recognition Based On Transformer</h3>
	   <strong>Da Chang</strong>, Yu Li <br />
           Prepint , 2024.4 <br />
            <p style="line-height: 1.8;">We explored the optimization of various full-parameter fine-tuning methods, such as LoRA in LLM. For OCR, a visual-text hybrid model, corresponding to the Transformer architecture, DoRA and LoRA have great improvements for visual encoders and text decoders in hybrid datasets including handwriting, print and Street View datasets, respectively.</p>
            <p><a href="https://arxiv.org/abs/2404.12734">paper</a>,<a href="https://github.com/VceChang/DLoRA-TrOCR">code</a></p>
        </td>
    </tr>
</table>

	
<h2>Research that I proudly participate in</h2>
<!-- <p>SfMDiffusion: Self-Supervised Monocular Depth Estimation in Endoscopy Based on Diffusion Models <br />
Yu Li, <b>Da Chang</b>, Jin Huang, Lan Dong, Du Wang, Liye Mei, Cheng Lei <br />
Under Review</p> -->

<table style="width: 100%; border-collapse: collapse;border: none;">
    <tr>
        <td style="width: 35%; text-align: center; vertical-align: middle; padding-right: 20px;border: none;">
            <img src="img/SfMD.jpg" alt="SfMD" style="max-width: 80%; height: auto;">
        </td>
         
        <td style="width: 65%; vertical-align: top;text-align: left;border: none;">
           <h3 style="line-height: 1.5;">SfMDiffusion: Self-Supervised Monocular Depth Estimation in Endoscopy Based on Diffusion Models</h3>
	   Yu Li, <b>Da Chang</b>, Jin Huang, Lan Dong, Du Wang, Liye Mei, Cheng Lei <br />
            Under Review , 2024.6 <br />
            <p style="line-height: 1.8;">For endoscope medical scenarios, we use the diffusion model for depth estimation. We build a teacher model, set knowledge distillation, optical appearance and ddim losses, and introduce the teacher's discriminative prior, which significantly enhances the accuracy and confidence of the results.</p>
            <a href="https://github.com/Skylanding/SfM-Diffusion">code</a></p>
        </td>
    </tr>
</table>



<h2>Honors and Awards </h2>
<ul><li><p>Second prize of the 8th National Biomedical Engineering Innovation Design Competition for College Students, China, 2023.</p></li></ul>
<ul><li><p>Third prize of the 9th National Statistical Modeling Competition for College Students, China, 2023.</p></li></ul>
<ul><li><p>Second Class Scholarship, CSU (Top 15%), 2023.</p></li></ul>
<ul><li><p>"ShanHe Excellent Student" Second Class Scholarship, CSU(Top 5%), 2022.</p></li></ul>
<ul><li><p>First-Class Scholarship, CSU (Top 5%), 2022.</p></li></ul>
</td>
</tr>
</table>
</body>
</html>
